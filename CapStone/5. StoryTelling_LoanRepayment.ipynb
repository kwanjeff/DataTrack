{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e766582",
   "metadata": {},
   "source": [
    "# StoryTelling - Predict likelyness of Client default on Loans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737001c5",
   "metadata": {},
   "source": [
    "copy start problem statement, where you got data \n",
    "\n",
    "data wrangling\n",
    "eda\n",
    "preprocess\n",
    "modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3e1ed",
   "metadata": {},
   "source": [
    "issue with data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42197a",
   "metadata": {},
   "source": [
    "charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ecc7d",
   "metadata": {},
   "source": [
    "add  hypothesis test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889212d",
   "metadata": {},
   "source": [
    "details from model exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6c8bd",
   "metadata": {},
   "source": [
    "As a bank I want to rate a customer based on each client's attributes.  We want to create our own predictive analysis model that tells me the likelihood that this customer will default on oneâ€™s credit card loan or will fully repay the loan.\n",
    "\n",
    "Criteria for success: \n",
    "If we are able to identify top 10-12 categories which leads to customer defalutness.  With the categories identified, am I able to accurately predict 85% clients that are flagged as likely to default.  Next step is what number of categories that can change prediction from 85% to 95% of flagged clients will default on loan.  If a lower amount of loan was given will it decrease the rate of default.\n",
    "\n",
    "\n",
    "The dataset used for this submission is found at https://www.kaggle.com/mishra5001/credit-card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faafad1",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669196de",
   "metadata": {},
   "source": [
    "The dataset found on kaggle was not large but the columns of features were quite extensive.  The first task I took was rename 2 columns.  Most of the columns were selfexplanitory or gave you a glimpse of what the data was about.  \n",
    "\n",
    "I had to dorp a lot of these features(columns) were dropped because the data was too sparse or unecessary.  This dataset had 122 features.  An exammple is that the following columns had 60% of data null.  In Task 2.3 I create a threshold of of 40% that mad missing data and they were automatically droppped.  \n",
    "\n",
    "In Task 2.2 there were exception where the data was missing but it could been explain that the car was new so instead of 1 year they left it blank.  The columnn that had that was 'OWN_CAR_AGE'.  Now the columns that we are keeping no longer have null values\n",
    "\n",
    "After dropping the columns that seem unneeded we were down to 76 columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd59b60",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a6f0a",
   "metadata": {},
   "source": [
    "I made several types of charts.  Here are a list of some of the plots i used. Box plot, Violin plot, heatmap, scatter plots, stacked bargraphs and pair plots\n",
    "\n",
    "Heatmap were are to decipher as the color were close and hard to distinguish.\n",
    "\n",
    "Box plots where almost indistinguishable in certain comparision.  It looked very garbled.\n",
    "\n",
    "I did several violin graphs which show some intersting insight One is live in a city vs urban areas and level of education.  The vast amount of people with Secondary and Higher Education were vast majority of people borrowing loands.  The graph also show as peole with higher level of education the smaller chances of default.  Acedemic degree almost had the least amount of default\n",
    "\n",
    "I found several features signification after hypothesis testing.  They are FLAG_OWN_REALTY, LIVE_CITY_NOT_WORK_CITY, and CODE_GENDER.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d99e8",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cd613",
   "metadata": {},
   "source": [
    "In this task we had to convert all columns from charcters to numbers.  One of the steps i used was convert these features into a more managable number.  I took the log10 of features 'AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_GOODS_PRICE', and 'AMT_ANNUITY'.  \n",
    "\n",
    "Next step was to use pd.get_dummies to convert char to numberic values. \n",
    "\n",
    "In this notebook i split the data based on the column 'Late_Payment' into test and train data.  It allows me to to sepearate the train and test data evenly so when we model it the data is not skewed.  Once the data was split, i scaled, transformed the data and saved the output to all the next step of modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffff6d",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e5c4e1",
   "metadata": {},
   "source": [
    "In modeling I used 3 different modeling methods.  They were gridsearch with logistic regression, Knn-neighbor with confussion matrix and area under curve with precission and recall.\n",
    "\n",
    "\n",
    "In the Gridsearch with Logistic Regression\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
